{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fceb7ece",
   "metadata": {},
   "source": [
    "**Text Analytics**\n",
    "1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d3eeb",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b544afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Kanchan\n",
      "[nltk_data]     Chintalwar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Kanchan\n",
      "[nltk_data]     Chintalwar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Kanchan\n",
      "[nltk_data]     Chintalwar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Kanchan\n",
      "[nltk_data]     Chintalwar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Kanchan\n",
      "[nltk_data]     Chintalwar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n",
      "[nltk_data] Error with downloaded zip file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb9c2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd288749",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./paragraph.txt') as f:\n",
    "    paragraph = f.read()\n",
    "    paragraph = paragraph.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d154abeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part-of-speech (pos) tagging is a popular natural language processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context.\\nlemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. lemmatization is similar to stemming but it brings context to the words.\\nstemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. stemming is important in natural language understanding (nlu) and natural language processing (nlp).'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be168213",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d64158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23c9655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of sentences in paragraph is  5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of sentences in paragraph is \", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f684152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are: - \n",
      " ['part-of-speech (pos) tagging is a popular natural language processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context.', 'lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item.', 'lemmatization is similar to stemming but it brings context to the words.', 'stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.', 'stemming is important in natural language understanding (nlu) and natural language processing (nlp).']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are: - \\n\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bd93751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of words in paragraph is  127\n",
      "Words are: - \n",
      " ['part-of-speech', '(', 'pos', ')', 'tagging', 'is', 'a', 'popular', 'natural', 'language', 'processing', 'process', 'which', 'refers', 'to', 'categorizing', 'words', 'in', 'a', 'text', '(', 'corpus', ')', 'in', 'correspondence', 'with', 'a', 'particular', 'part', 'of', 'speech', ',', 'depending', 'on', 'the', 'definition', 'of', 'the', 'word', 'and', 'its', 'context', '.', 'lemmatization', 'is', 'the', 'process', 'of', 'grouping', 'together', 'the', 'different', 'inflected', 'forms', 'of', 'a', 'word', 'so', 'they', 'can', 'be', 'analyzed', 'as', 'a', 'single', 'item', '.', 'lemmatization', 'is', 'similar', 'to', 'stemming', 'but', 'it', 'brings', 'context', 'to', 'the', 'words', '.', 'stemming', 'is', 'the', 'process', 'of', 'reducing', 'a', 'word', 'to', 'its', 'word', 'stem', 'that', 'affixes', 'to', 'suffixes', 'and', 'prefixes', 'or', 'to', 'the', 'roots', 'of', 'words', 'known', 'as', 'a', 'lemma', '.', 'stemming', 'is', 'important', 'in', 'natural', 'language', 'understanding', '(', 'nlu', ')', 'and', 'natural', 'language', 'processing', '(', 'nlp', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(paragraph)\n",
    "print(\"Total Number of words in paragraph is \", len(words))\n",
    "print(\"Words are: - \\n\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc0ab26",
   "metadata": {},
   "source": [
    "#### Full forms of all the abrevations used in POS Tagging"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8191443",
   "metadata": {},
   "source": [
    "CC coordinating conjunction \n",
    "CD cardinal digit \n",
    "DT determiner \n",
    "EX existential there (like: “there is” … think of it like “there exists”) \n",
    "FW foreign word \n",
    "IN preposition/subordinating conjunction \n",
    "JJ adjective – ‘big’ \n",
    "JJR adjective, comparative – ‘bigger’ \n",
    "JJS adjective, superlative – ‘biggest’ \n",
    "LS list marker 1) \n",
    "MD modal – could, will \n",
    "NN noun, singular ‘- desk’ \n",
    "NNS noun plural – ‘desks’ \n",
    "NNP proper noun, singular – ‘Harrison’ \n",
    "NNPS proper noun, plural – ‘Americans’ \n",
    "PDT predeterminer – ‘all the kids’ \n",
    "POS possessive ending parent’s \n",
    "PRP personal pronoun –  I, he, she \n",
    "PRP$ possessive pronoun – my, his, hers \n",
    "RB adverb – very, silently, \n",
    "RBR adverb, comparative – better \n",
    "RBS adverb, superlative – best \n",
    "RP particle – give up \n",
    "TO – to go ‘to’ the store. \n",
    "UH interjection – errrrrrrrm \n",
    "VB verb, base form – take \n",
    "VBD verb, past tense – took \n",
    "VBG verb, gerund/present participle – taking \n",
    "VBN verb, past participle – taken \n",
    "VBP verb, sing. present, non-3d – take \n",
    "VBZ verb, 3rd person sing. present – takes \n",
    "WDT wh-determiner – which \n",
    "WP wh-pronoun – who, what \n",
    "WP$ possessive wh-pronoun, eg- whose \n",
    "WRB wh-abverb, eg- where, when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828ff5a",
   "metadata": {},
   "source": [
    "## POS Tagging (Parts-of-speech tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24c0be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62a8e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagged form of words which are tokenize :\n",
      "('part-of-speech', 'NN')\n",
      "('(', '(')\n",
      "('pos', 'NN')\n",
      "(')', ')')\n",
      "('tagging', 'NN')\n",
      "('is', 'VBZ')\n",
      "('a', 'DT')\n",
      "('popular', 'JJ')\n",
      "('natural', 'JJ')\n",
      "('language', 'NN')\n",
      "('processing', 'NN')\n",
      "('process', 'NN')\n",
      "('which', 'WDT')\n",
      "('refers', 'VBZ')\n",
      "('to', 'TO')\n",
      "('categorizing', 'VBG')\n",
      "('words', 'NNS')\n",
      "('in', 'IN')\n",
      "('a', 'DT')\n",
      "('text', 'NN')\n",
      "('(', '(')\n",
      "('corpus', 'NN')\n",
      "(')', ')')\n",
      "('in', 'IN')\n",
      "('correspondence', 'NN')\n",
      "('with', 'IN')\n",
      "('a', 'DT')\n",
      "('particular', 'JJ')\n",
      "('part', 'NN')\n",
      "('of', 'IN')\n",
      "('speech', 'NN')\n",
      "(',', ',')\n",
      "('depending', 'VBG')\n",
      "('on', 'IN')\n",
      "('the', 'DT')\n",
      "('definition', 'NN')\n",
      "('of', 'IN')\n",
      "('the', 'DT')\n",
      "('word', 'NN')\n",
      "('and', 'CC')\n",
      "('its', 'PRP$')\n",
      "('context', 'NN')\n",
      "('.', '.')\n",
      "('lemmatization', 'NN')\n",
      "('is', 'VBZ')\n",
      "('the', 'DT')\n",
      "('process', 'NN')\n",
      "('of', 'IN')\n",
      "('grouping', 'VBG')\n",
      "('together', 'RB')\n",
      "('the', 'DT')\n",
      "('different', 'JJ')\n",
      "('inflected', 'JJ')\n",
      "('forms', 'NNS')\n",
      "('of', 'IN')\n",
      "('a', 'DT')\n",
      "('word', 'NN')\n",
      "('so', 'IN')\n",
      "('they', 'PRP')\n",
      "('can', 'MD')\n",
      "('be', 'VB')\n",
      "('analyzed', 'VBN')\n",
      "('as', 'IN')\n",
      "('a', 'DT')\n",
      "('single', 'JJ')\n",
      "('item', 'NN')\n",
      "('.', '.')\n",
      "('lemmatization', 'NN')\n",
      "('is', 'VBZ')\n",
      "('similar', 'JJ')\n",
      "('to', 'TO')\n",
      "('stemming', 'VBG')\n",
      "('but', 'CC')\n",
      "('it', 'PRP')\n",
      "('brings', 'VBZ')\n",
      "('context', 'NN')\n",
      "('to', 'TO')\n",
      "('the', 'DT')\n",
      "('words', 'NNS')\n",
      "('.', '.')\n",
      "('stemming', 'NN')\n",
      "('is', 'VBZ')\n",
      "('the', 'DT')\n",
      "('process', 'NN')\n",
      "('of', 'IN')\n",
      "('reducing', 'VBG')\n",
      "('a', 'DT')\n",
      "('word', 'NN')\n",
      "('to', 'TO')\n",
      "('its', 'PRP$')\n",
      "('word', 'NN')\n",
      "('stem', 'NN')\n",
      "('that', 'WDT')\n",
      "('affixes', 'VBZ')\n",
      "('to', 'TO')\n",
      "('suffixes', 'VB')\n",
      "('and', 'CC')\n",
      "('prefixes', 'VB')\n",
      "('or', 'CC')\n",
      "('to', 'TO')\n",
      "('the', 'DT')\n",
      "('roots', 'NNS')\n",
      "('of', 'IN')\n",
      "('words', 'NNS')\n",
      "('known', 'VBN')\n",
      "('as', 'IN')\n",
      "('a', 'DT')\n",
      "('lemma', 'NN')\n",
      "('.', '.')\n",
      "('stemming', 'NN')\n",
      "('is', 'VBZ')\n",
      "('important', 'JJ')\n",
      "('in', 'IN')\n",
      "('natural', 'JJ')\n",
      "('language', 'NN')\n",
      "('understanding', 'NN')\n",
      "('(', '(')\n",
      "('nlu', 'JJ')\n",
      "(')', ')')\n",
      "('and', 'CC')\n",
      "('natural', 'JJ')\n",
      "('language', 'NN')\n",
      "('processing', 'NN')\n",
      "('(', '(')\n",
      "('nlp', 'JJ')\n",
      "(')', ')')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "print('POS Tagged form of words which are tokenize :')\n",
    "for tag in tagged:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc1113",
   "metadata": {},
   "source": [
    "## Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d2e14d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "799eceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part-of-speech', '(', 'pos', ')', 'tagging', 'popular', 'natural', 'language', 'processing', 'process', 'refers', 'categorizing', 'words', 'text', '(', 'corpus', ')', 'correspondence', 'particular', 'part', 'speech', ',', 'depending', 'definition', 'word', 'context', '.', 'lemmatization', 'process', 'grouping', 'together', 'different', 'inflected', 'forms', 'word', 'analyzed', 'single', 'item', '.', 'lemmatization', 'similar', 'stemming', 'brings', 'context', 'words', '.', 'stemming', 'process', 'reducing', 'word', 'word', 'stem', 'affixes', 'suffixes', 'prefixes', 'roots', 'words', 'known', 'lemma', '.', 'stemming', 'important', 'natural', 'language', 'understanding', '(', 'nlu', ')', 'natural', 'language', 'processing', '(', 'nlp', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        word_tokens.append(word)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51967f",
   "metadata": {},
   "source": [
    "## POS Tagging after removal of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c40eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d15062f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagged form of words which are tokenize and from which stop words are removed:\n",
      "('part-of-speech', 'NN')\n",
      "('(', '(')\n",
      "('pos', 'NN')\n",
      "(')', ')')\n",
      "('tagging', 'VBG')\n",
      "('popular', 'JJ')\n",
      "('natural', 'JJ')\n",
      "('language', 'NN')\n",
      "('processing', 'NN')\n",
      "('process', 'NN')\n",
      "('refers', 'NNS')\n",
      "('categorizing', 'VBG')\n",
      "('words', 'NNS')\n",
      "('text', 'NN')\n",
      "('(', '(')\n",
      "('corpus', 'NN')\n",
      "(')', ')')\n",
      "('correspondence', 'NN')\n",
      "('particular', 'JJ')\n",
      "('part', 'NN')\n",
      "('speech', 'NN')\n",
      "(',', ',')\n",
      "('depending', 'VBG')\n",
      "('definition', 'NN')\n",
      "('word', 'NN')\n",
      "('context', 'NN')\n",
      "('.', '.')\n",
      "('lemmatization', 'NN')\n",
      "('process', 'NN')\n",
      "('grouping', 'VBG')\n",
      "('together', 'RB')\n",
      "('different', 'JJ')\n",
      "('inflected', 'JJ')\n",
      "('forms', 'NNS')\n",
      "('word', 'NN')\n",
      "('analyzed', 'VBN')\n",
      "('single', 'JJ')\n",
      "('item', 'NN')\n",
      "('.', '.')\n",
      "('lemmatization', 'NN')\n",
      "('similar', 'JJ')\n",
      "('stemming', 'NN')\n",
      "('brings', 'NNS')\n",
      "('context', 'JJ')\n",
      "('words', 'NNS')\n",
      "('.', '.')\n",
      "('stemming', 'VBG')\n",
      "('process', 'NN')\n",
      "('reducing', 'VBG')\n",
      "('word', 'NN')\n",
      "('word', 'NN')\n",
      "('stem', 'NN')\n",
      "('affixes', 'NNS')\n",
      "('suffixes', 'VBP')\n",
      "('prefixes', 'NNS')\n",
      "('roots', 'NNS')\n",
      "('words', 'NNS')\n",
      "('known', 'VBN')\n",
      "('lemma', 'NNS')\n",
      "('.', '.')\n",
      "('stemming', 'VBG')\n",
      "('important', 'JJ')\n",
      "('natural', 'JJ')\n",
      "('language', 'NN')\n",
      "('understanding', 'NN')\n",
      "('(', '(')\n",
      "('nlu', 'NN')\n",
      "(')', ')')\n",
      "('natural', 'JJ')\n",
      "('language', 'NN')\n",
      "('processing', 'NN')\n",
      "('(', '(')\n",
      "('nlp', 'JJ')\n",
      "(')', ')')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "print('POS Tagged form of words which are tokenize and from which stop words are removed:')\n",
    "for tag in tagged:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a60af0e",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e233d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5aff015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Stemming\n",
      "part-of-speech --> part-of-speech\n",
      "( --> (\n",
      "pos --> po\n",
      ") --> )\n",
      "tagging --> tag\n",
      "popular --> popular\n",
      "natural --> natur\n",
      "language --> languag\n",
      "processing --> process\n",
      "process --> process\n",
      "refers --> refer\n",
      "categorizing --> categor\n",
      "words --> word\n",
      "text --> text\n",
      "corpus --> corpu\n",
      "correspondence --> correspond\n",
      "particular --> particular\n",
      "part --> part\n",
      "speech --> speech\n",
      ", --> ,\n",
      "depending --> depend\n",
      "definition --> definit\n",
      "word --> word\n",
      "context --> context\n",
      ". --> .\n",
      "lemmatization --> lemmat\n",
      "grouping --> group\n",
      "together --> togeth\n",
      "different --> differ\n",
      "inflected --> inflect\n",
      "forms --> form\n",
      "analyzed --> analyz\n",
      "single --> singl\n",
      "item --> item\n",
      "similar --> similar\n",
      "stemming --> stem\n",
      "brings --> bring\n",
      "reducing --> reduc\n",
      "stem --> stem\n",
      "affixes --> affix\n",
      "suffixes --> suffix\n",
      "prefixes --> prefix\n",
      "roots --> root\n",
      "known --> known\n",
      "lemma --> lemma\n",
      "important --> import\n",
      "understanding --> understand\n",
      "nlu --> nlu\n",
      "nlp --> nlp\n"
     ]
    }
   ],
   "source": [
    "print('Results of Stemming')\n",
    "stemmed = {word: ps.stem(word) for word in word_tokens}\n",
    "for pair in stemmed.items():\n",
    "    print('{0} --> {1}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8dd591",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5b755f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f361e35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Lemmatization\n",
      "part-of-speech --> part-of-speech\n",
      "( --> (\n",
      "pos --> po\n",
      ") --> )\n",
      "tagging --> tagging\n",
      "popular --> popular\n",
      "natural --> natural\n",
      "language --> language\n",
      "processing --> processing\n",
      "process --> process\n",
      "refers --> refers\n",
      "categorizing --> categorizing\n",
      "words --> word\n",
      "text --> text\n",
      "corpus --> corpus\n",
      "correspondence --> correspondence\n",
      "particular --> particular\n",
      "part --> part\n",
      "speech --> speech\n",
      ", --> ,\n",
      "depending --> depending\n",
      "definition --> definition\n",
      "word --> word\n",
      "context --> context\n",
      ". --> .\n",
      "lemmatization --> lemmatization\n",
      "grouping --> grouping\n",
      "together --> together\n",
      "different --> different\n",
      "inflected --> inflected\n",
      "forms --> form\n",
      "analyzed --> analyzed\n",
      "single --> single\n",
      "item --> item\n",
      "similar --> similar\n",
      "stemming --> stemming\n",
      "brings --> brings\n",
      "reducing --> reducing\n",
      "stem --> stem\n",
      "affixes --> affix\n",
      "suffixes --> suffix\n",
      "prefixes --> prefix\n",
      "roots --> root\n",
      "known --> known\n",
      "lemma --> lemma\n",
      "important --> important\n",
      "understanding --> understanding\n",
      "nlu --> nlu\n",
      "nlp --> nlp\n"
     ]
    }
   ],
   "source": [
    "print('Results of Lemmatization')\n",
    "lemmatized = {word: lemmatizer.lemmatize(word) for word in word_tokens}\n",
    "for pair in lemmatized.items():\n",
    "    print('{0} --> {1}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3924de",
   "metadata": {},
   "source": [
    "### Term Frequency and Inverse Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bed7826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_convert_1d(arr):\n",
    "    arr = np.array(arr)\n",
    "    arr = np.concatenate( arr, axis=0 )\n",
    "    arr = np.concatenate( arr, axis=0 )\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1af509c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = []\n",
    "def cosine(trans):\n",
    "    cos.append(cosine_similarity(trans[0], trans[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01a71356",
   "metadata": {},
   "outputs": [],
   "source": [
    "manhatten = []\n",
    "def manhatten_distance(trans):\n",
    "    manhatten.append(pairwise_distances(trans[0], trans[1], metric = 'manhattan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f46e6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = []\n",
    "def euclidean_function(vectors):\n",
    "    euc=euclidean_distances(vectors[0], vectors[1])\n",
    "    euclidean.append(euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e5e4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(str1, str2):\n",
    "    vect = TfidfVectorizer()\n",
    "    vect.fit(word_tokens)\n",
    "    corpus = [str1,str2]\n",
    "    trans = vect.transform(corpus)\n",
    "    euclidean_function(trans)\n",
    "    cosine(trans)\n",
    "    manhatten_distance(trans)\n",
    "    return convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7861d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert():\n",
    "    dataf = pd.DataFrame()\n",
    "    lis2 = arr_c onvert_1d(manhatten)\n",
    "    dataf['manhatten'] = lis2\n",
    "    lis2 = arr_convert_1d(cos)\n",
    "    dataf['cos_sim'] = lis2\n",
    "    lis2 = arr_convert_1d(euclidean)\n",
    "    dataf['euclidean'] = lis2\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d44c9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   manhatten  cos_sim  euclidean\n",
      "0        1.0      0.0   1.000000\n",
      "1        2.0      0.0   1.414214\n"
     ]
    }
   ],
   "source": [
    "str1 = 'Stemming'\n",
    "str2 = 'Lemmatization'\n",
    "newData = tfidf(str1,str2);\n",
    "print(newData);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27367c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
